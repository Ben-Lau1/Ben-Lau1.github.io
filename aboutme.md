---
layout: page
title: Aboutme
# subtitle: Excerpt from Soulshaping by Jeff Brown
# cover-img: /assets/img/path.jpg
# thumbnail-img: /assets/img/thumb.png
# share-img: /assets/img/path.jpg
# tags: [books, test]
author: Lau Qinghe
---
My name is **Qinghe/Ben Lau (刘庆贺)**, a young researcher with a strong passion for 3D point cloud deep learning and its interdisciplinary applications. I graduated from Northwest Agriculture and Forestry University in July 2024 with a Bachelor's degree in Computer Science and Technology (GPA: 3.79/4.0), and my undergraduate thesis was recognized as an Excellent Undergraduate Thesis.

During my undergraduate studies, I developed a profound interest in computer vision, particularly in 3D point cloud processing and analysis. My academic journey began with implementing and comparing classical point cloud deep learning frameworks (such as PointNet++ and PAConv) on agricultural fruit point cloud datasets. This initial experience provided me with a foundational understanding of the research directions and technical challenges in point cloud deep learning.

Subsequently, as a team leader (of 5 members), I delved deeper into "Research on Segmentation and Recognition of Three-Dimensional Fruit Traits Based on Point Cloud Deep Learning" (06/2022 – 06/2024). In this project, we not only constructed a strawberry point cloud dataset under complex scenarios but also innovatively designed a density-based feature extraction and feature spreading network, **FSDNet**, tailored for agricultural picking scenarios, which significantly improved segmentation accuracy. This work has been published in *Computers and Electronics in Agriculture* (IF: 8.3). Furthermore, as a core member, I contributed to the "Design and Research of Point Cloud Neural Network Based on High-Order Feature Interaction" project, where we adapted the HorNet architecture from the image domain to point clouds, developing **Point HorNet**. This network achieved excellent performance on the Stanford indoor point cloud dataset, and the findings were accepted by the ICVR 2024 conference. My graduation thesis focused on optimizing point cloud grouping and sampling algorithms, where I designed a grid-based sampling and grouping scheme and a lightweight network, **GPFEnet**, substantially enhancing algorithmic efficiency. This research has been accepted by the IJCNN 2025 conference.

These research experiences have provided me with a comprehensive and in-depth understanding of the entire point cloud deep learning pipeline, from dataset construction, algorithm design and improvement, code implementation and debugging, to manuscript writing and publication. I am proficient in Python, C/C++, and CUDA C programming, adept with deep learning frameworks like PyTorch, and skilled in conducting research and development efficiently within a Linux environment.

I am particularly enthusiastic about the following research directions:
* **Efficient Point Cloud Processing Algorithms:** Including but not limited to segmentation, recognition, registration, and their robustness in complex scenes.
* **Lightweight Point Cloud Neural Network Design:** Exploring methods to reduce model complexity while maintaining high performance, to meet the demands of real-world applications (e.g., agricultural automation, robotic perception).
* **Fusion of 3D Vision with Other Modalities:** [If you have preliminary ideas, e.g., integrating RGB information, force feedback, etc., you can mention them here].
* **Cutting-edge Applications and Challenges of Deep Learning in [Specific Application Areas, e.g., Smart Agriculture, Autonomous Driving, 3D Reconstruction].**

I am eager to join a dynamic and innovative research team to pursue a PhD/MPhil degree, where I can continue my exploration at the forefront of 3D computer vision and deep learning. I am committed to applying research outcomes to solve complex real-world problems and look forward to making meaningful contributions to [the field you wish to contribute to].

Please feel free to explore my Projects and Publications sections on this website for more details.